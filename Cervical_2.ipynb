{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all necessary libraries to run the code\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import NaiveBayesClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "# using the variable sw to hold all stopwords that are in English\n",
    "sw = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>rating</th>\n",
       "      <th>past</th>\n",
       "      <th>stopwords_removal</th>\n",
       "      <th>reviewer</th>\n",
       "      <th>id</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>fee</th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "      <th>...</th>\n",
       "      <th>stopwords_removal_nltk</th>\n",
       "      <th>present_simple</th>\n",
       "      <th>dataSource</th>\n",
       "      <th>appId</th>\n",
       "      <th>date</th>\n",
       "      <th>sentiScore_pos</th>\n",
       "      <th>present_con</th>\n",
       "      <th>length_words</th>\n",
       "      <th>stopwords_removal_lemmatization</th>\n",
       "      <th>Exclude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Besides the occasional crash, this is an amazi...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>besides occasional crash, this amazing product...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>264</td>\n",
       "      <td>besid the occa crash, thi is an amaz produc wi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Almost perfect</td>\n",
       "      <td>Bug</td>\n",
       "      <td>...</td>\n",
       "      <td>besides occasional crash, amazing product tons...</td>\n",
       "      <td>2</td>\n",
       "      <td>RE2014_app_and_play_store_apps</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>besides occasional crash, this amaze product w...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  rating  past  \\\n",
       "0  Besides the occasional crash, this is an amazi...       5     0   \n",
       "\n",
       "                                   stopwords_removal reviewer   id  \\\n",
       "0  besides occasional crash, this amazing product...      NaN  264   \n",
       "\n",
       "                                             stemmed  fee           title  \\\n",
       "0  besid the occa crash, thi is an amaz produc wi...  NaN  Almost perfect   \n",
       "\n",
       "  label   ...                               stopwords_removal_nltk  \\\n",
       "0   Bug   ...    besides occasional crash, amazing product tons...   \n",
       "\n",
       "  present_simple                      dataSource  appId  date sentiScore_pos  \\\n",
       "0              2  RE2014_app_and_play_store_apps    NaN   NaN              3   \n",
       "\n",
       "   present_con length_words  \\\n",
       "0            1           22   \n",
       "\n",
       "                     stopwords_removal_lemmatization Exclude  \n",
       "0  besides occasional crash, this amaze product w...     NaN  \n",
       "\n",
       "[1 rows x 25 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"all.csv\") \n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3691, 25)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    2250\n",
       "4     626\n",
       "1     342\n",
       "3     262\n",
       "2     211\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>rating</th>\n",
       "      <th>past</th>\n",
       "      <th>stopwords_removal</th>\n",
       "      <th>reviewer</th>\n",
       "      <th>id</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>fee</th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "      <th>...</th>\n",
       "      <th>stopwords_removal_nltk</th>\n",
       "      <th>present_simple</th>\n",
       "      <th>dataSource</th>\n",
       "      <th>appId</th>\n",
       "      <th>date</th>\n",
       "      <th>sentiScore_pos</th>\n",
       "      <th>present_con</th>\n",
       "      <th>length_words</th>\n",
       "      <th>stopwords_removal_lemmatization</th>\n",
       "      <th>Exclude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Besides the occasional crash, this is an amazi...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>besides occasional crash, this amazing product...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>264</td>\n",
       "      <td>besid the occa crash, thi is an amaz produc wi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Almost perfect</td>\n",
       "      <td>Bug</td>\n",
       "      <td>...</td>\n",
       "      <td>besides occasional crash, amazing product tons...</td>\n",
       "      <td>2</td>\n",
       "      <td>RE2014_app_and_play_store_apps</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>besides occasional crash, this amaze product w...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>This could be a great app if it was predictabl...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>this could be great app if was predictable, bu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>111</td>\n",
       "      <td>thi could be a gre ap if it was predictable, b...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Take a photo of your boarding pass</td>\n",
       "      <td>Bug</td>\n",
       "      <td>...</td>\n",
       "      <td>could great app predictable, full bugs unpredi...</td>\n",
       "      <td>9</td>\n",
       "      <td>AppStore_Random</td>\n",
       "      <td>382698565</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>this could be great app if be predictable, but...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  rating  past  \\\n",
       "0  Besides the occasional crash, this is an amazi...       5     0   \n",
       "1  This could be a great app if it was predictabl...       1     1   \n",
       "\n",
       "                                   stopwords_removal reviewer   id  \\\n",
       "0  besides occasional crash, this amazing product...      NaN  264   \n",
       "1  this could be great app if was predictable, bu...      NaN  111   \n",
       "\n",
       "                                             stemmed  fee  \\\n",
       "0  besid the occa crash, thi is an amaz produc wi...  NaN   \n",
       "1  thi could be a gre ap if it was predictable, b...  NaN   \n",
       "\n",
       "                                title label   ...    \\\n",
       "0                      Almost perfect   Bug   ...     \n",
       "1  Take a photo of your boarding pass   Bug   ...     \n",
       "\n",
       "                              stopwords_removal_nltk present_simple  \\\n",
       "0  besides occasional crash, amazing product tons...              2   \n",
       "1  could great app predictable, full bugs unpredi...              9   \n",
       "\n",
       "                       dataSource      appId  date sentiScore_pos  \\\n",
       "0  RE2014_app_and_play_store_apps        NaN   NaN              3   \n",
       "1                 AppStore_Random  382698565   NaN              3   \n",
       "\n",
       "   present_con length_words  \\\n",
       "0            1           22   \n",
       "1            0           58   \n",
       "\n",
       "                     stopwords_removal_lemmatization Exclude  \n",
       "0  besides occasional crash, this amaze product w...     NaN  \n",
       "1  this could be great app if be predictable, but...     NaN  \n",
       "\n",
       "[2 rows x 25 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndf= data.groupby('rating').head(211)\n",
    "df = ndf.loc[ndf['rating'].isin([1,2,3,4,5])]\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>rating</th>\n",
       "      <th>past</th>\n",
       "      <th>stopwords_removal</th>\n",
       "      <th>reviewer</th>\n",
       "      <th>id</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>fee</th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "      <th>...</th>\n",
       "      <th>stopwords_removal_nltk</th>\n",
       "      <th>present_simple</th>\n",
       "      <th>dataSource</th>\n",
       "      <th>appId</th>\n",
       "      <th>date</th>\n",
       "      <th>sentiScore_pos</th>\n",
       "      <th>present_con</th>\n",
       "      <th>length_words</th>\n",
       "      <th>stopwords_removal_lemmatization</th>\n",
       "      <th>Exclude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Besides occasional crash, amazing product tons...</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>besides occasional crash, this amazing product...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>264</td>\n",
       "      <td>besid the occa crash, thi is an amaz produc wi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Almost perfect</td>\n",
       "      <td>Bug</td>\n",
       "      <td>...</td>\n",
       "      <td>besides occasional crash, amazing product tons...</td>\n",
       "      <td>2</td>\n",
       "      <td>RE2014_app_and_play_store_apps</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>besides occasional crash, this amaze product w...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment  rating  past  \\\n",
       "0  Besides occasional crash, amazing product tons...       5     0   \n",
       "\n",
       "                                   stopwords_removal reviewer   id  \\\n",
       "0  besides occasional crash, this amazing product...      NaN  264   \n",
       "\n",
       "                                             stemmed  fee           title  \\\n",
       "0  besid the occa crash, thi is an amaz produc wi...  NaN  Almost perfect   \n",
       "\n",
       "  label   ...                               stopwords_removal_nltk  \\\n",
       "0   Bug   ...    besides occasional crash, amazing product tons...   \n",
       "\n",
       "  present_simple                      dataSource  appId  date sentiScore_pos  \\\n",
       "0              2  RE2014_app_and_play_store_apps    NaN   NaN              3   \n",
       "\n",
       "   present_con length_words  \\\n",
       "0            1           22   \n",
       "\n",
       "                     stopwords_removal_lemmatization Exclude  \n",
       "0  besides occasional crash, this amaze product w...     NaN  \n",
       "\n",
       "[1 rows x 25 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#applying pre-processing steps to remove stopwords and words of size less than 2\n",
    "df['comment'] = df['comment'].apply(lambda x: x.split())\n",
    "wordsEng = stopwords.words('english')\n",
    "df['comment'] = df['comment'].apply(lambda x:[item for item in x if item not in wordsEng])\n",
    "df['comment'] = df['comment'].apply(lambda x: [w for w in x if len(w)>2])\n",
    "df['comment'] = df['comment'].apply(lambda x: \" \".join(x))\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the variable sw to hold all stopwords that are in English\n",
    "sw = stopwords.words('english')\n",
    "TEXT=[]\n",
    "#Rating=[]\n",
    "for i in range(len(df)):\n",
    "    #if data['cetagory'][i]=='Racing':\n",
    "    review = re.sub('[^a-zA-Z]', ' ',df['comment'][i])\n",
    "    #review = re.sub('[/(){}\\[\\]\\|@!,;]', ' ',data['reviews'][i])\n",
    "    #review = re.sub('[^0-9a-zA-Z #+_♥️]', ' ',data['reviews'][i])#Remove bad symbols\n",
    "    review = re.sub(r'\\d+', '',review)\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "\n",
    "    review = [token for token in review if token not in sw]\n",
    "    review=' '.join(review)\n",
    "    TEXT.append(review)\n",
    "    #Rating.append(data['rating'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.rating\n",
    "X = TEXT\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,1))\n",
    "X_train= vectorizer.fit_transform(X_train)\n",
    "X_test=vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'comment', 'rating', 'past', 'stopwords_removal', 'reviewer',\n",
       "       'id', 'stemmed', 'fee', 'title', 'label', 'future',\n",
       "       'lemmatized_comment', 'sentiScore', 'sentiScore_neg', 'reviewId',\n",
       "       'stopwords_removal_nltk', 'present_simple', 'dataSource', 'appId',\n",
       "       'date', 'sentiScore_pos', 'present_con', 'length_words',\n",
       "       'stopwords_removal_lemmatization', 'Exclude'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-fdbac1dde340>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1650\u001b[0m         \"\"\"\n\u001b[0;32m   1651\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1652\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1653\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1654\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1056\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m-> 1058\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m   1059\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1060\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    968\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    969\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 970\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    971\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    350\u001b[0m                                                tokenize)\n\u001b[0;32m    351\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 352\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m             raise ValueError(\"np.nan is an invalid document, expected byte or \"\n\u001b[0m\u001b[0;32m    144\u001b[0m                              \"unicode string.\")\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "y = data.rating\n",
    "X = data.title\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,3))\n",
    "X_train= vectorizer.fit_transform(X_train)\n",
    "X_test=vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    2250\n",
       "4     626\n",
       "1     342\n",
       "3     262\n",
       "2     211\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=[]\n",
    "\n",
    "predictions.append(predsETC)\n",
    "predictions.append(predSGD)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ETC  SGD\n",
      "0       5    5\n",
      "1       5    5\n",
      "2       5    5\n",
      "3       5    5\n",
      "4       5    5\n",
      "5       5    1\n",
      "6       5    5\n",
      "7       1    1\n",
      "8       5    5\n",
      "9       5    5\n",
      "10      5    4\n",
      "11      5    5\n",
      "12      5    5\n",
      "13      5    5\n",
      "14      5    5\n",
      "15      5    5\n",
      "16      4    5\n",
      "17      5    5\n",
      "18      5    5\n",
      "19      5    5\n",
      "20      4    4\n",
      "21      5    5\n",
      "22      5    5\n",
      "23      5    5\n",
      "24      5    5\n",
      "25      5    5\n",
      "26      5    5\n",
      "27      5    5\n",
      "28      5    5\n",
      "29      4    5\n",
      "...   ...  ...\n",
      "1078    5    5\n",
      "1079    5    5\n",
      "1080    5    5\n",
      "1081    5    5\n",
      "1082    1    4\n",
      "1083    5    5\n",
      "1084    5    5\n",
      "1085    5    5\n",
      "1086    5    5\n",
      "1087    5    5\n",
      "1088    5    5\n",
      "1089    5    5\n",
      "1090    5    5\n",
      "1091    5    5\n",
      "1092    5    5\n",
      "1093    5    4\n",
      "1094    5    5\n",
      "1095    5    5\n",
      "1096    5    5\n",
      "1097    5    5\n",
      "1098    5    5\n",
      "1099    5    5\n",
      "1100    3    4\n",
      "1101    5    5\n",
      "1102    5    5\n",
      "1103    5    5\n",
      "1104    5    5\n",
      "1105    5    4\n",
      "1106    1    1\n",
      "1107    5    5\n",
      "\n",
      "[1108 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "df1 = DataFrame(predictions).transpose()\n",
    "df1.columns=['ETC','SGD']\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Final_prediction'] = df1.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RF</th>\n",
       "      <th>GBM</th>\n",
       "      <th>LR</th>\n",
       "      <th>GNB</th>\n",
       "      <th>ETC</th>\n",
       "      <th>SGD</th>\n",
       "      <th>VC</th>\n",
       "      <th>Final_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RF  GBM  LR  GNB  ETC  SGD  VC  Final_prediction\n",
       "0   5    5   5    5    5    5   5               5.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "MeanPredictions = df1['Final_prediction'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "MeanPredictions=list(np.int_(MeanPredictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5.0,\n",
       " 4.142857142857143,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 3.4285714285714284,\n",
       " 5.0,\n",
       " 2.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.714285714285714,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.857142857142857,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.285714285714286,\n",
       " 5.0,\n",
       " 4.857142857142857,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.714285714285714,\n",
       " 5.0,\n",
       " 4.714285714285714,\n",
       " 4.285714285714286,\n",
       " 3.142857142857143,\n",
       " 5.0,\n",
       " 3.857142857142857,\n",
       " 1.5714285714285714,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.714285714285714,\n",
       " 5.0,\n",
       " 2.5714285714285716,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.428571428571429,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 3.2857142857142856,\n",
       " 5.0,\n",
       " 1.0,\n",
       " 5.0,\n",
       " 1.4285714285714286,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 3.2857142857142856,\n",
       " 5.0,\n",
       " 4.285714285714286,\n",
       " 5.0,\n",
       " 4.571428571428571,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.714285714285714,\n",
       " 5.0,\n",
       " 4.857142857142857,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.571428571428571,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.857142857142857,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 2.7142857142857144,\n",
       " 4.857142857142857,\n",
       " 5.0,\n",
       " 4.428571428571429,\n",
       " 4.714285714285714,\n",
       " 5.0,\n",
       " 2.5714285714285716,\n",
       " 4.428571428571429,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 1.5714285714285714,\n",
       " 5.0,\n",
       " 4.571428571428571,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.857142857142857,\n",
       " 4.428571428571429,\n",
       " 5.0,\n",
       " 1.0,\n",
       " 4.857142857142857,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.285714285714286,\n",
       " 4.285714285714286,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 3.857142857142857,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.428571428571429,\n",
       " 4.571428571428571,\n",
       " 5.0,\n",
       " 4.285714285714286,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 1.4285714285714286,\n",
       " 4.142857142857143,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.428571428571429,\n",
       " 5.0,\n",
       " 4.714285714285714,\n",
       " 5.0,\n",
       " 4.857142857142857,\n",
       " 5.0,\n",
       " 1.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 2.4285714285714284,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 1.0,\n",
       " 4.285714285714286,\n",
       " 5.0,\n",
       " 1.8571428571428572,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 1.0,\n",
       " 5.0,\n",
       " 2.7142857142857144,\n",
       " 5.0,\n",
       " 2.142857142857143,\n",
       " 5.0,\n",
       " 1.4285714285714286,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 2.142857142857143,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 3.2857142857142856,\n",
       " 4.571428571428571,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.285714285714286,\n",
       " 5.0,\n",
       " 1.5714285714285714,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 2.142857142857143,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.714285714285714,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.428571428571429,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.857142857142857,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.714285714285714,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 1.5714285714285714,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 4.714285714285714,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.428571428571429,\n",
       " 5.0,\n",
       " 4.714285714285714,\n",
       " 4.714285714285714,\n",
       " 4.857142857142857,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 2.7142857142857144,\n",
       " 1.0,\n",
       " 4.857142857142857,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.285714285714286,\n",
       " 5.0,\n",
       " 4.714285714285714,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.142857142857143,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.857142857142857,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.857142857142857,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 3.5714285714285716,\n",
       " 4.142857142857143,\n",
       " 4.857142857142857,\n",
       " 5.0,\n",
       " 4.857142857142857,\n",
       " 4.571428571428571,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.857142857142857,\n",
       " 4.714285714285714,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.428571428571429,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.857142857142857,\n",
       " 3.5714285714285716,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 1.5714285714285714,\n",
       " 2.7142857142857144,\n",
       " 4.428571428571429,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.285714285714286,\n",
       " 5.0,\n",
       " 4.571428571428571,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.714285714285714,\n",
       " 3.857142857142857,\n",
       " 4.857142857142857,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.714285714285714,\n",
       " 4.857142857142857,\n",
       " 1.5714285714285714,\n",
       " 5.0,\n",
       " 4.428571428571429,\n",
       " 5.0,\n",
       " 4.714285714285714,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 2.142857142857143,\n",
       " 3.7142857142857144,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 2.4285714285714284,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 4.857142857142857,\n",
       " 4.857142857142857,\n",
       " 4.714285714285714,\n",
       " 5.0,\n",
       " 4.428571428571429,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 3.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 2.4285714285714284,\n",
       " 2.2857142857142856,\n",
       " 4.857142857142857,\n",
       " 4.857142857142857,\n",
       " 4.571428571428571,\n",
       " 1.4285714285714286,\n",
       " 4.857142857142857,\n",
       " 5.0,\n",
       " 2.7142857142857144,\n",
       " 5.0,\n",
       " 3.0,\n",
       " 5.0,\n",
       " 3.857142857142857,\n",
       " 5.0,\n",
       " 4.428571428571429,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 3.857142857142857,\n",
       " 4.428571428571429,\n",
       " 5.0,\n",
       " 4.428571428571429,\n",
       " 5.0,\n",
       " 4.857142857142857,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.428571428571429,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.857142857142857,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 2.142857142857143,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 4.714285714285714,\n",
       " 5.0,\n",
       " 4.857142857142857,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.428571428571429,\n",
       " 5.0,\n",
       " 2.7142857142857144,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.714285714285714,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 3.2857142857142856,\n",
       " 4.142857142857143,\n",
       " 4.571428571428571,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.714285714285714,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.285714285714286,\n",
       " 4.428571428571429,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.857142857142857,\n",
       " 4.714285714285714,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.285714285714286,\n",
       " 5.0,\n",
       " 4.857142857142857,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.428571428571429,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.428571428571429,\n",
       " 4.857142857142857,\n",
       " 2.5714285714285716,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.428571428571429,\n",
       " 5.0,\n",
       " 4.142857142857143,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 2.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.857142857142857,\n",
       " 5.0,\n",
       " 4.714285714285714,\n",
       " 5.0,\n",
       " 4.428571428571429,\n",
       " 4.142857142857143,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.857142857142857,\n",
       " 4.857142857142857,\n",
       " 5.0,\n",
       " 4.714285714285714,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.428571428571429,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 3.7142857142857144,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 3.857142857142857,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.428571428571429,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 3.857142857142857,\n",
       " 5.0,\n",
       " 4.428571428571429,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 3.7142857142857144,\n",
       " 4.428571428571429,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 1.5714285714285714,\n",
       " 5.0,\n",
       " 4.571428571428571,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 1.5714285714285714,\n",
       " 5.0,\n",
       " 3.0,\n",
       " 5.0,\n",
       " 4.857142857142857,\n",
       " 5.0,\n",
       " 4.428571428571429,\n",
       " 5.0,\n",
       " 4.857142857142857,\n",
       " 4.714285714285714,\n",
       " 4.428571428571429,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.285714285714286,\n",
       " 4.857142857142857,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.714285714285714,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 3.5714285714285716,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 1.4285714285714286,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.857142857142857,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.714285714285714,\n",
       " 5.0,\n",
       " 3.2857142857142856,\n",
       " 5.0,\n",
       " 1.5714285714285714,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.428571428571429,\n",
       " 2.142857142857143,\n",
       " 5.0,\n",
       " 1.5714285714285714,\n",
       " 5.0,\n",
       " 1.5714285714285714,\n",
       " 5.0,\n",
       " 4.142857142857143,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 1.5714285714285714,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.428571428571429,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.428571428571429,\n",
       " 1.4285714285714286,\n",
       " 4.857142857142857,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 3.857142857142857,\n",
       " 4.714285714285714,\n",
       " 5.0,\n",
       " 3.2857142857142856,\n",
       " 5.0,\n",
       " 2.7142857142857144,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 2.857142857142857,\n",
       " 1.2857142857142858,\n",
       " 2.142857142857143,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.714285714285714,\n",
       " 4.857142857142857,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.428571428571429,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.857142857142857,\n",
       " 4.714285714285714,\n",
       " 4.857142857142857,\n",
       " 4.714285714285714,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.142857142857143,\n",
       " 4.857142857142857,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.714285714285714,\n",
       " 2.2857142857142856,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.857142857142857,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.857142857142857,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 3.2857142857142856,\n",
       " 5.0,\n",
       " 3.5714285714285716,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.142857142857143,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 2.4285714285714284,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.571428571428571,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.428571428571429,\n",
       " 1.5714285714285714,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.714285714285714,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 3.142857142857143,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.714285714285714,\n",
       " 5.0,\n",
       " 3.4285714285714284,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.571428571428571,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.571428571428571,\n",
       " 5.0,\n",
       " 3.7142857142857144,\n",
       " 4.428571428571429,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.571428571428571,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 3.4285714285714284,\n",
       " 4.714285714285714,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.857142857142857,\n",
       " 4.142857142857143,\n",
       " 5.0,\n",
       " 2.2857142857142856,\n",
       " 4.571428571428571,\n",
       " 1.8571428571428572,\n",
       " 3.857142857142857,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 1.5714285714285714,\n",
       " 1.5714285714285714,\n",
       " 5.0,\n",
       " 2.7142857142857144,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.714285714285714,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 3.2857142857142856,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.857142857142857,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 1.0,\n",
       " 2.857142857142857,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.142857142857143,\n",
       " 5.0,\n",
       " 2.5714285714285716,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.857142857142857,\n",
       " 5.0,\n",
       " 4.428571428571429,\n",
       " 5.0,\n",
       " 4.285714285714286,\n",
       " 4.285714285714286,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 1.5714285714285714,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.428571428571429,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.857142857142857,\n",
       " 4.714285714285714,\n",
       " 5.0,\n",
       " 4.571428571428571,\n",
       " 4.285714285714286,\n",
       " 3.2857142857142856,\n",
       " 4.285714285714286,\n",
       " 4.857142857142857,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 1.5714285714285714,\n",
       " 3.7142857142857144,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.428571428571429,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.285714285714286,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 1.4285714285714286,\n",
       " 4.571428571428571,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 3.142857142857143,\n",
       " 4.714285714285714,\n",
       " 5.0,\n",
       " 4.714285714285714,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.714285714285714,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.714285714285714,\n",
       " 5.0,\n",
       " 3.2857142857142856,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 1.5714285714285714,\n",
       " 4.857142857142857,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.571428571428571,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.142857142857143,\n",
       " 5.0,\n",
       " 1.5714285714285714,\n",
       " 4.857142857142857,\n",
       " 1.5714285714285714,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 3.857142857142857,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 3.857142857142857,\n",
       " 3.2857142857142856,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 3.2857142857142856,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.714285714285714,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.571428571428571,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.285714285714286,\n",
       " 2.7142857142857144,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 3.4285714285714284,\n",
       " 4.285714285714286,\n",
       " 1.7142857142857142,\n",
       " 3.2857142857142856,\n",
       " 5.0,\n",
       " 4.0,\n",
       " 5.0,\n",
       " 4.142857142857143,\n",
       " 4.714285714285714,\n",
       " 4.857142857142857,\n",
       " 2.857142857142857,\n",
       " 1.0,\n",
       " 4.428571428571429,\n",
       " 4.428571428571429,\n",
       " 5.0,\n",
       " 3.857142857142857,\n",
       " 4.285714285714286,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.714285714285714,\n",
       " 5.0,\n",
       " 4.857142857142857,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.142857142857143,\n",
       " 4.428571428571429,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 2.4285714285714284,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.428571428571429,\n",
       " 4.857142857142857,\n",
       " 4.714285714285714,\n",
       " 5.0,\n",
       " 4.571428571428571,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.857142857142857,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.428571428571429,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.857142857142857,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.857142857142857,\n",
       " 2.142857142857143,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 4.285714285714286,\n",
       " 4.428571428571429,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 2.5714285714285716,\n",
       " 4.0,\n",
       " 1.8571428571428572,\n",
       " 5.0,\n",
       " 2.5714285714285716,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 1.4285714285714286,\n",
       " 4.142857142857143,\n",
       " 3.2857142857142856,\n",
       " 4.714285714285714,\n",
       " 4.428571428571429,\n",
       " 2.0,\n",
       " 1.7142857142857142,\n",
       " 4.428571428571429,\n",
       " 5.0,\n",
       " 3.857142857142857,\n",
       " ...]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AveragePredictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2,2) ,min_df=1, use_idf=True, smooth_idf=True)\n",
    "# Vectorize the training data\n",
    "ADASYSNVector = vectorizer.fit_transform(data.comment)\n",
    "# Vectorize the test data\n",
    "#X_test = vectorizer.transform(X_test)\n",
    "X=ADASYSNVector\n",
    "y=data.rating\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "# transform the dataset\n",
    "X, y = ADASYN().fit_sample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADASYN Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#balanced Vector\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(2,2) ,min_df=1, use_idf=True, smooth_idf=True)\n",
    "# Vectorize the training data\n",
    "SmoteVector = vectorizer.fit_transform(data.comment)\n",
    "# Vectorize the test data\n",
    "#X_test = vectorizer.transform(X_test)\n",
    "X=SmoteVector\n",
    "y=data.rating\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "# transform the dataset\n",
    "oversample = SMOTE()\n",
    "X, y = oversample.fit_resample(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smote Train Test Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muhammad Umer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Muhammad Umer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "0.8598518518518519\n",
      "Extra Tree\n",
      "0.8882962962962963\n",
      "GBM\n",
      "0.6767407407407408\n",
      "LR\n",
      "0.8628148148148148\n",
      "NB\n",
      "0.7454814814814815\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF TextBlob\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "rfc = RandomForestClassifier(n_estimators=100,n_jobs=-1,random_state=2)\n",
    "etc = ExtraTreesClassifier(n_estimators=100,n_jobs=-1,random_state=2)\n",
    "gb = GradientBoostingClassifier(n_estimators=100, learning_rate = 0.1,random_state=2)\n",
    "clf= LogisticRegression()\n",
    "gnb = BernoulliNB()\n",
    "preds = rfc.fit(X_train, y_train).predict(X_test)\n",
    "predsGB = gb.fit(X_train, y_train).predict(X_test)\n",
    "predsLR = clf.fit(X_train, y_train).predict(X_test)\n",
    "predsGNB = gnb.fit(X_train, y_train).predict(X_test)\n",
    "predsETC = etc.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Random Forest\")\n",
    "print(accuracy_score(y_test, preds))\n",
    "print(\"Extra Tree\")\n",
    "print(accuracy_score(y_test, predsETC))\n",
    "print(\"GBM\")\n",
    "print(accuracy_score(y_test, predsGB))\n",
    "print(\"LR\")\n",
    "print(accuracy_score(y_test, predsLR))\n",
    "print(\"NB\")\n",
    "print(accuracy_score(y_test, predsGNB))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muhammad Umer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Muhammad Umer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "0.8725229222123632\n",
      "Extra Tree\n",
      "0.9012126589766342\n",
      "GBM\n",
      "0.6648920437740313\n",
      "LR\n",
      "0.8636498077491866\n",
      "NB\n",
      "0.7521443359952676\n"
     ]
    }
   ],
   "source": [
    "#ADASYN\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "rfc = RandomForestClassifier(n_estimators=100,n_jobs=-1,random_state=2)\n",
    "etc = ExtraTreesClassifier(n_estimators=100,n_jobs=-1,random_state=2)\n",
    "gb = GradientBoostingClassifier(n_estimators=100, learning_rate = 0.1,random_state=2)\n",
    "clf= LogisticRegression()\n",
    "gnb = BernoulliNB()\n",
    "preds = rfc.fit(X_train, y_train).predict(X_test)\n",
    "predsGB = gb.fit(X_train, y_train).predict(X_test)\n",
    "predsLR = clf.fit(X_train, y_train).predict(X_test)\n",
    "predsGNB = gnb.fit(X_train, y_train).predict(X_test)\n",
    "predsETC = etc.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Random Forest\")\n",
    "print(accuracy_score(y_test, preds))\n",
    "print(\"Extra Tree\")\n",
    "print(accuracy_score(y_test, predsETC))\n",
    "print(\"GBM\")\n",
    "print(accuracy_score(y_test, predsGB))\n",
    "print(\"LR\")\n",
    "print(accuracy_score(y_test, predsLR))\n",
    "print(\"NB\")\n",
    "print(accuracy_score(y_test, predsGNB))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muhammad Umer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Muhammad Umer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "0.674187725631769\n",
      "Extra Tree\n",
      "0.6796028880866426\n",
      "GBM\n",
      "0.6570397111913358\n",
      "LR\n",
      "0.6498194945848376\n",
      "NB\n",
      "0.6272563176895307\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF TextBlob\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "rfc = RandomForestClassifier(n_estimators=100,n_jobs=-1,random_state=2)\n",
    "etc = ExtraTreesClassifier(n_estimators=100,n_jobs=-1,random_state=2)\n",
    "gb = GradientBoostingClassifier(n_estimators=100, learning_rate = 0.1,random_state=2)\n",
    "clf= LogisticRegression()\n",
    "gnb = BernoulliNB()\n",
    "preds = rfc.fit(X_train, y_train).predict(X_test)\n",
    "predsGB = gb.fit(X_train, y_train).predict(X_test)\n",
    "predsLR = clf.fit(X_train, y_train).predict(X_test)\n",
    "predsGNB = gnb.fit(X_train, y_train).predict(X_test)\n",
    "predsETC = etc.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Random Forest\")\n",
    "print(accuracy_score(y_test, preds))\n",
    "print(\"Extra Tree\")\n",
    "print(accuracy_score(y_test, predsETC))\n",
    "print(\"GBM\")\n",
    "print(accuracy_score(y_test, predsGB))\n",
    "print(\"LR\")\n",
    "print(accuracy_score(y_test, predsLR))\n",
    "print(\"NB\")\n",
    "print(accuracy_score(y_test, predsGNB))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muhammad Umer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Muhammad Umer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "0.3312302839116719\n",
      "Extra Tree\n",
      "0.31545741324921134\n",
      "GBM\n",
      "0.3438485804416404\n",
      "LR\n",
      "0.34069400630914826\n",
      "NB\n",
      "0.31545741324921134\n",
      "Stochastic Gradient Classifier\n",
      "0.31545741324921134\n",
      "Voting Classifier LR+SGD\n",
      "0.3438485804416404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muhammad Umer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Muhammad Umer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#Balanced under-sampling\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "rfc = RandomForestClassifier(n_estimators=100,n_jobs=-1,random_state=2)\n",
    "etc = ExtraTreesClassifier(n_estimators=100,n_jobs=-1,random_state=2)\n",
    "gb = GradientBoostingClassifier(n_estimators=100, learning_rate = 0.1,random_state=2)\n",
    "clf= LogisticRegression()\n",
    "gnb = BernoulliNB()\n",
    "predsBi = rfc.fit(X_train, y_train).predict(X_test)\n",
    "predsGBBi = gb.fit(X_train, y_train).predict(X_test)\n",
    "predsLRBi = clf.fit(X_train, y_train).predict(X_test)\n",
    "predsGNBBi = gnb.fit(X_train, y_train).predict(X_test)\n",
    "predsETCBi = etc.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Random Forest\")\n",
    "print(accuracy_score(y_test, predsBi))\n",
    "print(\"Extra Tree\")\n",
    "print(accuracy_score(y_test, predsETCBi))\n",
    "print(\"GBM\")\n",
    "print(accuracy_score(y_test, predsGBBi))\n",
    "print(\"LR\")\n",
    "print(accuracy_score(y_test, predsLRBi))\n",
    "print(\"NB\")\n",
    "print(accuracy_score(y_test, predsGNBBi))\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "print(\"Stochastic Gradient Classifier\")\n",
    "clf = SGDClassifier(max_iter=1100, tol=1e-3)\n",
    "calibrated_clf = CalibratedClassifierCV(clf, cv=5, method='isotonic')\n",
    "predSGD=calibrated_clf.fit(X_train, y_train).predict(X_test)\n",
    "print(accuracy_score(y_test,predSGD))\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "print(\"Voting Classifier LR+SGD\")\n",
    "clf2 = SGDClassifier(max_iter=1100, tol=1e-3)\n",
    "clf1 = LogisticRegression()\n",
    "eclf1 = VotingClassifier(estimators=[\n",
    "        ('lr', clf1), ('sgd', clf2)],voting='hard')\n",
    "predictionVC=eclf1.fit(X_train, y_train).predict(X_test)\n",
    "print(accuracy_score(y_test,predictionVC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Classifier\n",
      "0.912\n",
      "Voting Classifier LR+SGD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muhammad Umer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Muhammad Umer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8711111111111111\n"
     ]
    }
   ],
   "source": [
    "#SMOTE\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "print(\"Stochastic Gradient Classifier\")\n",
    "clf = SGDClassifier(max_iter=1100, tol=1e-3)\n",
    "calibrated_clf = CalibratedClassifierCV(clf, cv=5, method='isotonic')\n",
    "predSGD=calibrated_clf.fit(X_train, y_train).predict(X_test)\n",
    "print(accuracy_score(y_test,predSGD))\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "print(\"Voting Classifier LR+SGD\")\n",
    "clf2 = SGDClassifier(max_iter=1100, tol=1e-3)\n",
    "clf1 = LogisticRegression()\n",
    "eclf1 = VotingClassifier(estimators=[\n",
    "        ('lr', clf1), ('sgd', clf2)],voting='hard')\n",
    "predictionVC=eclf1.fit(X_train, y_train).predict(X_test)\n",
    "print(accuracy_score(y_test,predictionVC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic Gradient Classifier\n",
      "0.9174800354924578\n",
      "Voting Classifier LR+SGD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muhammad Umer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\Muhammad Umer\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8796214137829045\n"
     ]
    }
   ],
   "source": [
    "#ADASYN\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "print(\"Stochastic Gradient Classifier\")\n",
    "clf = SGDClassifier(max_iter=1100, tol=1e-3)\n",
    "calibrated_clf = CalibratedClassifierCV(clf, cv=5, method='isotonic')\n",
    "predSGD=calibrated_clf.fit(X_train, y_train).predict(X_test)\n",
    "print(accuracy_score(y_test,predSGD))\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "print(\"Voting Classifier LR+SGD\")\n",
    "clf2 = SGDClassifier(max_iter=1100, tol=1e-3)\n",
    "clf1 = LogisticRegression()\n",
    "eclf1 = VotingClassifier(estimators=[\n",
    "        ('lr', clf1), ('sgd', clf2)],voting='hard')\n",
    "predictionVC=eclf1.fit(X_train, y_train).predict(X_test)\n",
    "print(accuracy_score(y_test,predictionVC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.49      0.29      0.37        89\n",
      "           2       0.44      0.06      0.10        71\n",
      "           3       0.31      0.05      0.09        81\n",
      "           4       0.42      0.13      0.20       186\n",
      "           5       0.69      0.98      0.81       681\n",
      "\n",
      "    accuracy                           0.66      1108\n",
      "   macro avg       0.47      0.30      0.31      1108\n",
      "weighted avg       0.58      0.66      0.57      1108\n",
      "\n",
      "GNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.51      0.28      0.36        89\n",
      "           2       0.22      0.03      0.05        71\n",
      "           3       0.33      0.02      0.05        81\n",
      "           4       0.32      0.12      0.18       186\n",
      "           5       0.66      0.94      0.78       681\n",
      "\n",
      "    accuracy                           0.63      1108\n",
      "   macro avg       0.41      0.28      0.28      1108\n",
      "weighted avg       0.54      0.63      0.54      1108\n",
      "\n",
      "ETC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.46      0.53      0.49        89\n",
      "           2       0.50      0.04      0.08        71\n",
      "           3       0.31      0.10      0.15        81\n",
      "           4       0.63      0.24      0.35       186\n",
      "           5       0.72      0.95      0.82       681\n",
      "\n",
      "    accuracy                           0.68      1108\n",
      "   macro avg       0.52      0.37      0.38      1108\n",
      "weighted avg       0.64      0.68      0.62      1108\n",
      "\n",
      "LR\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.50      0.36      0.42        89\n",
      "           2       0.00      0.00      0.00        71\n",
      "           3       0.00      0.00      0.00        81\n",
      "           4       0.59      0.05      0.10       186\n",
      "           5       0.66      1.00      0.79       681\n",
      "\n",
      "    accuracy                           0.65      1108\n",
      "   macro avg       0.35      0.28      0.26      1108\n",
      "weighted avg       0.55      0.65      0.54      1108\n",
      "\n",
      "SGD\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.55      0.63      0.59        89\n",
      "           2       0.75      0.04      0.08        71\n",
      "           3       0.50      0.09      0.15        81\n",
      "           4       0.52      0.27      0.35       186\n",
      "           5       0.73      0.95      0.82       681\n",
      "\n",
      "    accuracy                           0.69      1108\n",
      "   macro avg       0.61      0.40      0.40      1108\n",
      "weighted avg       0.66      0.69      0.63      1108\n",
      "\n",
      "Voting Classifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.49      0.65      0.56        89\n",
      "           2       0.31      0.07      0.11        71\n",
      "           3       0.33      0.12      0.18        81\n",
      "           4       0.43      0.28      0.34       186\n",
      "           5       0.76      0.92      0.83       681\n",
      "\n",
      "    accuracy                           0.68      1108\n",
      "   macro avg       0.46      0.41      0.41      1108\n",
      "weighted avg       0.62      0.68      0.63      1108\n",
      "\n",
      "Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.45      0.43      0.44        89\n",
      "           2       1.00      0.06      0.11        71\n",
      "           3       0.26      0.09      0.13        81\n",
      "           4       0.69      0.19      0.30       186\n",
      "           5       0.70      0.97      0.82       681\n",
      "\n",
      "    accuracy                           0.67      1108\n",
      "   macro avg       0.62      0.35      0.36      1108\n",
      "weighted avg       0.67      0.67      0.60      1108\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muhammad Umer\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF TextBlob\n",
    "print(\"GBM\")\n",
    "print(classification_report(y_test,predsGB))\n",
    "print(\"GNB\")\n",
    "print(classification_report(y_test,predsGNB))\n",
    "print(\"ETC\")\n",
    "print(classification_report(y_test,predsETC))\n",
    "print(\"LR\")\n",
    "print(classification_report(y_test,predsLR))\n",
    "print(\"SGD\")\n",
    "print(classification_report(y_test,predSGD))\n",
    "print(\"Voting Classifier\")\n",
    "print(classification_report(y_test,predictionVC))\n",
    "print(\"Random Forest\")\n",
    "print(classification_report(y_test,preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.71      0.79       673\n",
      "           1       0.90      0.77      0.83       665\n",
      "           2       0.92      0.57      0.71       701\n",
      "           3       0.41      0.78      0.54       690\n",
      "           4       0.67      0.55      0.60       646\n",
      "\n",
      "    accuracy                           0.68      3375\n",
      "   macro avg       0.76      0.68      0.69      3375\n",
      "weighted avg       0.76      0.68      0.69      3375\n",
      "\n",
      "GNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.77      0.87       673\n",
      "           1       1.00      0.77      0.87       665\n",
      "           2       0.95      0.64      0.76       701\n",
      "           3       0.93      0.60      0.73       690\n",
      "           4       0.44      0.97      0.60       646\n",
      "\n",
      "    accuracy                           0.75      3375\n",
      "   macro avg       0.86      0.75      0.77      3375\n",
      "weighted avg       0.87      0.75      0.77      3375\n",
      "\n",
      "ETC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.96       673\n",
      "           1       0.98      0.93      0.96       665\n",
      "           2       0.93      0.92      0.92       701\n",
      "           3       0.83      0.84      0.84       690\n",
      "           4       0.75      0.79      0.77       646\n",
      "\n",
      "    accuracy                           0.89      3375\n",
      "   macro avg       0.89      0.89      0.89      3375\n",
      "weighted avg       0.89      0.89      0.89      3375\n",
      "\n",
      "LR\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.98      0.85       673\n",
      "           1       0.95      0.92      0.94       665\n",
      "           2       0.96      0.90      0.93       701\n",
      "           3       0.84      0.81      0.82       690\n",
      "           4       0.84      0.69      0.76       646\n",
      "\n",
      "    accuracy                           0.86      3375\n",
      "   macro avg       0.87      0.86      0.86      3375\n",
      "weighted avg       0.87      0.86      0.86      3375\n",
      "\n",
      "SGD\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       673\n",
      "           1       0.97      0.96      0.97       665\n",
      "           2       0.97      0.93      0.95       701\n",
      "           3       0.93      0.81      0.86       690\n",
      "           4       0.74      0.89      0.81       646\n",
      "\n",
      "    accuracy                           0.91      3375\n",
      "   macro avg       0.92      0.91      0.91      3375\n",
      "weighted avg       0.92      0.91      0.91      3375\n",
      "\n",
      "Voting Classifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.99      0.85       673\n",
      "           1       0.95      0.94      0.94       665\n",
      "           2       0.96      0.91      0.94       701\n",
      "           3       0.87      0.82      0.85       690\n",
      "           4       0.87      0.68      0.76       646\n",
      "\n",
      "    accuracy                           0.87      3375\n",
      "   macro avg       0.88      0.87      0.87      3375\n",
      "weighted avg       0.88      0.87      0.87      3375\n",
      "\n",
      "Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.94       673\n",
      "           1       0.98      0.92      0.95       665\n",
      "           2       0.90      0.89      0.89       701\n",
      "           3       0.81      0.78      0.80       690\n",
      "           4       0.68      0.78      0.73       646\n",
      "\n",
      "    accuracy                           0.86      3375\n",
      "   macro avg       0.86      0.86      0.86      3375\n",
      "weighted avg       0.87      0.86      0.86      3375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#SMOTE\n",
    "print(\"GBM\")\n",
    "print(classification_report(y_test,predsGB))\n",
    "print(\"GNB\")\n",
    "print(classification_report(y_test,predsGNB))\n",
    "print(\"ETC\")\n",
    "print(classification_report(y_test,predsETC))\n",
    "print(\"LR\")\n",
    "print(classification_report(y_test,predsLR))\n",
    "print(\"SGD\")\n",
    "print(classification_report(y_test,predSGD))\n",
    "print(\"Voting Classifier\")\n",
    "print(classification_report(y_test,predictionVC))\n",
    "print(\"Random Forest\")\n",
    "print(classification_report(y_test,preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.71      0.79       696\n",
      "           1       0.88      0.74      0.80       631\n",
      "           2       0.90      0.55      0.69       704\n",
      "           3       0.41      0.78      0.53       707\n",
      "           4       0.69      0.54      0.61       643\n",
      "\n",
      "    accuracy                           0.66      3381\n",
      "   macro avg       0.75      0.66      0.68      3381\n",
      "weighted avg       0.75      0.66      0.68      3381\n",
      "\n",
      "GNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.78      0.88       696\n",
      "           1       1.00      0.74      0.85       631\n",
      "           2       0.96      0.62      0.75       704\n",
      "           3       0.91      0.68      0.78       707\n",
      "           4       0.45      0.96      0.61       643\n",
      "\n",
      "    accuracy                           0.75      3381\n",
      "   macro avg       0.86      0.76      0.77      3381\n",
      "weighted avg       0.87      0.75      0.77      3381\n",
      "\n",
      "ETC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96       696\n",
      "           1       0.98      0.96      0.97       631\n",
      "           2       0.95      0.92      0.93       704\n",
      "           3       0.84      0.89      0.86       707\n",
      "           4       0.79      0.78      0.78       643\n",
      "\n",
      "    accuracy                           0.90      3381\n",
      "   macro avg       0.90      0.90      0.90      3381\n",
      "weighted avg       0.90      0.90      0.90      3381\n",
      "\n",
      "LR\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.95      0.95       696\n",
      "           1       0.97      0.91      0.94       631\n",
      "           2       0.98      0.89      0.93       704\n",
      "           3       0.67      0.89      0.76       707\n",
      "           4       0.83      0.67      0.74       643\n",
      "\n",
      "    accuracy                           0.86      3381\n",
      "   macro avg       0.88      0.86      0.87      3381\n",
      "weighted avg       0.88      0.86      0.87      3381\n",
      "\n",
      "SGD\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97       696\n",
      "           1       0.97      0.96      0.96       631\n",
      "           2       0.98      0.93      0.95       704\n",
      "           3       0.94      0.84      0.89       707\n",
      "           4       0.75      0.88      0.81       643\n",
      "\n",
      "    accuracy                           0.92      3381\n",
      "   macro avg       0.92      0.92      0.92      3381\n",
      "weighted avg       0.92      0.92      0.92      3381\n",
      "\n",
      "Voting Classifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.99      0.88       696\n",
      "           1       0.96      0.96      0.96       631\n",
      "           2       0.97      0.92      0.94       704\n",
      "           3       0.85      0.86      0.85       707\n",
      "           4       0.86      0.66      0.75       643\n",
      "\n",
      "    accuracy                           0.88      3381\n",
      "   macro avg       0.89      0.88      0.88      3381\n",
      "weighted avg       0.88      0.88      0.88      3381\n",
      "\n",
      "Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93       696\n",
      "           1       0.98      0.93      0.95       631\n",
      "           2       0.94      0.88      0.91       704\n",
      "           3       0.82      0.84      0.83       707\n",
      "           4       0.72      0.79      0.75       643\n",
      "\n",
      "    accuracy                           0.87      3381\n",
      "   macro avg       0.88      0.87      0.87      3381\n",
      "weighted avg       0.88      0.87      0.87      3381\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ADASYN\n",
    "print(\"GBM\")\n",
    "print(classification_report(y_test,predsGB))\n",
    "print(\"GNB\")\n",
    "print(classification_report(y_test,predsGNB))\n",
    "print(\"ETC\")\n",
    "print(classification_report(y_test,predsETC))\n",
    "print(\"LR\")\n",
    "print(classification_report(y_test,predsLR))\n",
    "print(\"SGD\")\n",
    "print(classification_report(y_test,predSGD))\n",
    "print(\"Voting Classifier\")\n",
    "print(classification_report(y_test,predictionVC))\n",
    "print(\"Random Forest\")\n",
    "print(classification_report(y_test,preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.46      0.42      0.44        71\n",
      "           2       0.33      0.29      0.31        59\n",
      "           3       0.24      0.33      0.28        60\n",
      "           4       0.31      0.29      0.30        65\n",
      "           5       0.40      0.37      0.39        62\n",
      "\n",
      "    accuracy                           0.34       317\n",
      "   macro avg       0.35      0.34      0.34       317\n",
      "weighted avg       0.35      0.34      0.35       317\n",
      "\n",
      "GNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.56      0.21      0.31        71\n",
      "           2       0.40      0.20      0.27        59\n",
      "           3       0.16      0.15      0.15        60\n",
      "           4       0.42      0.17      0.24        65\n",
      "           5       0.30      0.85      0.45        62\n",
      "\n",
      "    accuracy                           0.32       317\n",
      "   macro avg       0.37      0.32      0.28       317\n",
      "weighted avg       0.37      0.32      0.28       317\n",
      "\n",
      "ETC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.39      0.52      0.45        71\n",
      "           2       0.24      0.25      0.25        59\n",
      "           3       0.20      0.15      0.17        60\n",
      "           4       0.26      0.17      0.21        65\n",
      "           5       0.38      0.45      0.41        62\n",
      "\n",
      "    accuracy                           0.32       317\n",
      "   macro avg       0.30      0.31      0.30       317\n",
      "weighted avg       0.30      0.32      0.30       317\n",
      "\n",
      "LR\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.48      0.62      0.54        71\n",
      "           2       0.23      0.24      0.24        59\n",
      "           3       0.17      0.13      0.15        60\n",
      "           4       0.27      0.23      0.25        65\n",
      "           5       0.43      0.44      0.43        62\n",
      "\n",
      "    accuracy                           0.34       317\n",
      "   macro avg       0.32      0.33      0.32       317\n",
      "weighted avg       0.32      0.34      0.33       317\n",
      "\n",
      "SGD\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.40      0.49      0.44        71\n",
      "           2       0.22      0.19      0.20        59\n",
      "           3       0.20      0.20      0.20        60\n",
      "           4       0.29      0.18      0.22        65\n",
      "           5       0.39      0.48      0.43        62\n",
      "\n",
      "    accuracy                           0.32       317\n",
      "   macro avg       0.30      0.31      0.30       317\n",
      "weighted avg       0.30      0.32      0.30       317\n",
      "\n",
      "Voting Classifier\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.44      0.62      0.52        71\n",
      "           2       0.22      0.24      0.23        59\n",
      "           3       0.22      0.25      0.23        60\n",
      "           4       0.35      0.22      0.27        65\n",
      "           5       0.47      0.35      0.40        62\n",
      "\n",
      "    accuracy                           0.34       317\n",
      "   macro avg       0.34      0.34      0.33       317\n",
      "weighted avg       0.35      0.34      0.34       317\n",
      "\n",
      "Random Forest\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.59      0.49        71\n",
      "           2       0.33      0.27      0.30        59\n",
      "           3       0.16      0.15      0.15        60\n",
      "           4       0.29      0.18      0.23        65\n",
      "           5       0.37      0.42      0.39        62\n",
      "\n",
      "    accuracy                           0.33       317\n",
      "   macro avg       0.31      0.32      0.31       317\n",
      "weighted avg       0.32      0.33      0.32       317\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Under-Sampling\n",
    "print(\"GBM\")\n",
    "print(classification_report(y_test,predsGBBi))\n",
    "print(\"GNB\")\n",
    "print(classification_report(y_test,predsGNBBi))\n",
    "print(\"ETC\")\n",
    "print(classification_report(y_test,predsETCBi))\n",
    "print(\"LR\")\n",
    "print(classification_report(y_test,predsLRBi))\n",
    "print(\"SGD\")\n",
    "print(classification_report(y_test,predSGD))\n",
    "print(\"Voting Classifier\")\n",
    "print(classification_report(y_test,predictionVC))\n",
    "print(\"Random Forest\")\n",
    "print(classification_report(y_test,predsBi))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Balanced under-sampling\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "rfc = RandomForestClassifier(n_estimators=100,n_jobs=-1,random_state=2)\n",
    "etc = ExtraTreesClassifier(n_estimators=100,n_jobs=-1,random_state=2)\n",
    "gb = GradientBoostingClassifier(n_estimators=100, learning_rate = 0.1,random_state=2)\n",
    "clf= LogisticRegression()\n",
    "gnb = BernoulliNB()\n",
    "predsBi = rfc.fit(X_train, y_train).predict(X_test)\n",
    "predsGBBi = gb.fit(X_train, y_train).predict(X_test)\n",
    "predsLRBi = clf.fit(X_train, y_train).predict(X_test)\n",
    "predsGNBBi = gnb.fit(X_train, y_train).predict(X_test)\n",
    "predsETCBi = etc.fit(X_train, y_train).predict(X_test)\n",
    "print(\"Random Forest\")\n",
    "print(accuracy_score(y_test, predsBi))\n",
    "print(\"Extra Tree\")\n",
    "print(accuracy_score(y_test, predsETCBi))\n",
    "print(\"GBM\")\n",
    "print(accuracy_score(y_test, predsGBBi))\n",
    "print(\"LR\")\n",
    "print(accuracy_score(y_test, predsLRBi))\n",
    "print(\"NB\")\n",
    "print(accuracy_score(y_test, predsGNBBi))\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "print(\"Stochastic Gradient Classifier\")\n",
    "clf = SGDClassifier(max_iter=1100, tol=1e-3)\n",
    "calibrated_clf = CalibratedClassifierCV(clf, cv=5, method='isotonic')\n",
    "predSGD=calibrated_clf.fit(X_train, y_train).predict(X_test)\n",
    "print(accuracy_score(y_test,predSGD))\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "print(\"Voting Classifier LR+SGD\")\n",
    "clf2 = SGDClassifier(max_iter=1100, tol=1e-3)\n",
    "clf1 = LogisticRegression()\n",
    "eclf1 = VotingClassifier(estimators=[\n",
    "        ('lr', clf1), ('sgd', clf2)],voting='hard')\n",
    "predictionVC=eclf1.fit(X_train, y_train).predict(X_test)\n",
    "print(accuracy_score(y_test,predictionVC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Muhammad Umer\\Anaconda3\\lib\\site-packages\\nltk\\twitter\\__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "Sentiment=[]\n",
    "\n",
    "from textblob import TextBlob\n",
    "# the variable 'message_text' now contains the text we will analyze.\n",
    "for i in range(len(data)):   \n",
    "    #message_text = 'i hate you'\n",
    "\n",
    "#print(message_text)\n",
    "\n",
    "# Calling the polarity_scores method on sid and passing in the message_text outputs a dictionary with negative, neutral, positive, and compound scores for the input text\n",
    "    b = TextBlob(data.text[i])\n",
    "    #scores = sid.polarity_scores(data.reviews[i])\n",
    "    value=b.sentiment[0]\n",
    "    if value<=0:\n",
    "        Sentiment.append(0)\n",
    "    else:\n",
    "        Sentiment.append(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# importing all necessary libraries to run the code\n",
    "import re,string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras_metrics\n",
    "import tensorflow.keras\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation,Embedding,GlobalMaxPooling1D\n",
    "# using the variable sw to hold all stopwords that are in English\n",
    "sw = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kahruveldesign wrote jan phones ringing need g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>please share warning popularity https co wbycg...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  kahruveldesign wrote jan phones ringing need g...\n",
       "1  please share warning popularity https co wbycg..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converting twitterSentiment[] list into dataframe for serving it to keras tokenizer\n",
    "dataSetFinal = pd.DataFrame(np.array(TEXT))\n",
    "dataSetFinal.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  0\n",
       "1  0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataSentiment = pd.DataFrame(np.array(Sentiment))\n",
    "dataSentiment.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tensorflow.keras.preprocessing.text.Tokenizer(num_words=2500, lower=True,split=' ',filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(dataSetFinal[0].values)\n",
    "#print(tokenizer.word_index)  # To see the dicstionary\n",
    "X = tokenizer.texts_to_sequences(dataSetFinal[0].values)\n",
    "X = tensorflow.keras.preprocessing.sequence.pad_sequences(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 23, 100)           250000    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 23, 64)            32064     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 11, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 11, 64)            20544     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 304,754\n",
      "Trainable params: 304,754\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "weight_decay = 1e-4\n",
    "#Deep Learning Network Structure\n",
    "model = Sequential()\n",
    "model.add(Embedding(2500,100, input_length=X.shape[1]))\n",
    "model.add(Conv1D(64, 5, activation='relu', padding='same'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Conv1D(64, 5, activation='relu', padding='same'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=['accuracy','mae','mse',keras_metrics.precision(), keras_metrics.recall()])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8300 samples, validate on 3558 samples\n",
      "Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "#CNN\n",
    "batch_size=64\n",
    "Y = pd.get_dummies(dataSentiment[0]).values\n",
    "X_train, X_valid, Y_train, Y_valid = train_test_split(X,Y, test_size = 0.30)\n",
    "#Here we train the Network.\n",
    "pred=model.fit(X_train, Y_train, batch_size =batch_size, epochs =50, verbose =2,validation_data=(X_valid,Y_valid))\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score=[]\n",
    "score=model.evaluate(X_valid,Y_valid,verbose=2,batch_size=batch_size)\n",
    "#keras.metrics.binary_accuracy(Y_valid,pred)\n",
    "print(\"score: %.2f\" %(score[0]))\n",
    "print(\"validation accuracy: %.2f\" % (score[1]))\n",
    "print(\"recall: %.2f\" %(score[4]))\n",
    "print(\"Precision: %.2f\" % (score[3]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
